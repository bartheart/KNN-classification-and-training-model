{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-_ekoGkEDlV"
      },
      "source": [
        "Course:CSCE 5215 Machine Learning\n",
        "\n",
        "Professor: Zeenat Tariq\n",
        "\n",
        "Week1 Day-2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets have a quick review"
      ],
      "metadata": {
        "id": "e9OhjApzkXoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image(url=\"https://www.researchgate.net/publication/354960266/figure/fig1/AS:1075175843983363@1633353305883/The-main-types-of-machine-learning-Main-approaches-include-classification-and.png\")"
      ],
      "metadata": {
        "id": "-fPo2qPskew2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numpy:\n",
        "\n",
        "1) A fundamental library within scientific computing.\n",
        "\n",
        "2) Central package of libraries, including scikit-learn, matplotlib, and pandas.\n",
        "\n",
        "3) Designed for multi-dimensional arrays, optimizing performance for fast array operations."
      ],
      "metadata": {
        "id": "-ugmnKleqyms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "sXE-9k68rN2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr_1d = np.array([1,2,3])\n",
        "print (f\"Create an array: {arr_1d}\")\n",
        "print(f\"Shape: {arr_1d.shape}\")\n",
        "print(f\"Dimention: {arr_1d.ndim}\")\n",
        "print(f\"Data type: {arr_1d.dtype}\")\n",
        "print(f\"Total number of elements: {arr_1d.size}\")\n",
        "print(f\"Size(in byte): {arr_1d.itemsize}\")\n",
        "print(f\"Access different elements: {arr_1d[0]}\")"
      ],
      "metadata": {
        "id": "dhLzN_SdrHkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) array/ matrix\n",
        "\n",
        "2) Mathematic operation\n",
        "\n",
        "3) Random numbers\n"
      ],
      "metadata": {
        "id": "CixZMS9WrqM8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pandas:\n",
        "\n"
      ],
      "metadata": {
        "id": "gTetbcMKq7en"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/omairaasim/machine_learning/master/project_11_k_nearest_neighbor/iphone_purchase_records.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "5pfTbBK8r7vU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[:,:].head()"
      ],
      "metadata": {
        "id": "V06ZlH7xsuKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[:5,:-1]"
      ],
      "metadata": {
        "id": "-Yp2dE8nstVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOvDcVBSC5ev"
      },
      "source": [
        "First Activity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Leyf34miC4-U"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# def age_gender_analysis(inp:str) -> None:\n",
        "#     \"\"\"\n",
        "#     Analyze age and gender to identify potential buyers.\n",
        "\n",
        "#     Parameter\n",
        "#     ---------\n",
        "#     inp:\n",
        "#       The gender for which age analysis will be performed.\n",
        "\n",
        "#     \"\"\"\n",
        "#     data = df.loc[df[\"Gender\"] == inp]\n",
        "#     min_age = np.min(data[\"Age\"])\n",
        "#     max_age = np.max(data[\"Age\"])\n",
        "#     print (f\"Young {inp}\")\n",
        "#     print (data.loc[data[\"Age\"]== min_age])\n",
        "#     print (\"-\"*5)\n",
        "#     print (f\"Old {inp}\")\n",
        "#     print (data.loc[data[\"Age\"]== max_age])\n",
        "\n",
        "# # df = pd.read_csv(\"https://raw.githubusercontent.com/omairaasim/machine_learning/master/project_11_k_nearest_neighbor/iphone_purchase_records.csv\")\n",
        "# df.rename(columns={\"Salary\":\"Income\"}, inplace=True)\n",
        "# stat = df[\"Gender\"].value_counts()\n",
        "# print (stat)\n",
        "# print (\"-\"*5)\n",
        "# for inp in [\"Female\", \"Male\"]:\n",
        "#     age_gender_analysis(inp)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us learn about the package -Scikit-Learn\n"
      ],
      "metadata": {
        "id": "48sv5TWljP2-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34a0g7My2TPf"
      },
      "source": [
        "Scikit-learn, also known as sklearn, is a popular open-source machine learning library in Python. It provides a wide range of tools and algorithms for various machine learning tasks, including classification, regression, clustering, dimensionality reduction, model selection, and preprocessing of data.\n",
        "\n",
        "Sklearn is built on top of other scientific computing libraries in Python, such as NumPy, SciPy, and matplotlib, and provides a consistent and user-friendly interface for working with machine learning algorithms. It aims to provide accessible and efficient implementations of state-of-the-art machine learning techniques and tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZ9OumKByqjZ"
      },
      "outputs": [],
      "source": [
        "# import the package\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1BZKnh6RyvP"
      },
      "outputs": [],
      "source": [
        "# check the version\n",
        "sklearn.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1VKnrnYVvcQ"
      },
      "outputs": [],
      "source": [
        "# check the sklearn propertes and what it offers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lC_nBsbWAAi"
      },
      "outputs": [],
      "source": [
        "# check datasets\n",
        "from sklearn import datasets\n",
        "dir(datasets)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image(url=\"https://static.packt-cdn.com/products/9781785880902/graphics/B04971_06_01.jpg\")"
      ],
      "metadata": {
        "id": "-M6T4KJJwBLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using the iris dataset which is loaded into our datafame in the link below.The Iris dataset is a popular dataset in machine learning and is often used for classification tasks. It contains measurements of four features (sepal length, sepal width, petal length, and petal width) of three different species of Iris flowers (Setosa, Versicolor, and Virginica)."
      ],
      "metadata": {
        "id": "SeLmsMNTtPRC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5oh2pFTV_74"
      },
      "outputs": [],
      "source": [
        "# import only what you need\n",
        "from sklearn.datasets import load_iris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csdptQoyy1La"
      },
      "source": [
        "To use the Iris dataset in scikit-learn, you need to import the dataset from the sklearn.datasets module. the load_iris() function is called from the sklearn.datasets module, and the returned dataset is assigned to the variable iris. The iris variable now holds the Iris dataset, which consists of the input features in the iris.data attribute and the corresponding target labels in the iris.target attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMDCeh4cV_5g"
      },
      "outputs": [],
      "source": [
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "iris"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUEFhWTBZEAb"
      },
      "outputs": [],
      "source": [
        "# See features names\n",
        "iris.feature_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEL1MERtyt-0"
      },
      "outputs": [],
      "source": [
        "# Extract the features and target variable\n",
        "X = iris.data\n",
        "y = iris.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjrJDmft-YuG"
      },
      "source": [
        "In the Iris dataset, the target variable is the \"species\" of the iris flowers. It has three classes or labels: \"setosa,\" \"versicolor,\" and \"virginica.\" Each sample in the dataset is associated with one of these three species."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cs0bfWlGX69y"
      },
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIMY5t_Y6Ipd"
      },
      "source": [
        "2a.Find the mean, minimum, and maximum values of the sepal length feature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PQqP8Kc6FOg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "sepal_length = X[:, 0]  # Extract sepal length feature\n",
        "mean_sepal_length = np.mean(sepal_length)\n",
        "min_sepal_length = np.min(sepal_length)\n",
        "max_sepal_length = np.max(sepal_length)\n",
        "\n",
        "print(\"Mean sepal length:\", mean_sepal_length)\n",
        "print(\"Minimum sepal length:\", min_sepal_length)\n",
        "print(\"Maximum sepal length:\", max_sepal_length)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNxHx414zNDa"
      },
      "source": [
        "2b.Calculate the covariance matrix of the features:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYSzhoAz2hq4"
      },
      "source": [
        "A covariance matrix is a square matrix that summarizes the covariance between multiple variables. It provides information about the relationships and the extent to which variables change together. In the context of statistics and data analysis, covariance measures how two variables vary or move together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QG1AxXFbjaTS"
      },
      "outputs": [],
      "source": [
        "# positive number: two variables tend to increase or decrease in tandem.\n",
        "# negative number: as one variable increases, a second variable tends to decrease."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LA_uBNYozN6E"
      },
      "outputs": [],
      "source": [
        "cov_matrix = np.cov(X.T)\n",
        "\n",
        "print(\"Covariance matrix:\")\n",
        "print(cov_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFV-P_yT2UOJ"
      },
      "outputs": [],
      "source": [
        "cov_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzuzkqitjRYT"
      },
      "outputs": [],
      "source": [
        "# Why do we need to transpose?\n",
        "# 1) Simplifies interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hk02jdyXctH2"
      },
      "outputs": [],
      "source": [
        "cov_matrix = np.cov(X)\n",
        "\n",
        "print(\"Covariance matrix:\")\n",
        "print(cov_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8943j5hczHA"
      },
      "outputs": [],
      "source": [
        "# What are these values?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wbyjg34N2QPS"
      },
      "outputs": [],
      "source": [
        "cov_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fUUxa6Phunm"
      },
      "outputs": [],
      "source": [
        "# Seaborn is a Python data visualization library\n",
        "# Matplotlib is a plotting library\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.heatmap(cov_matrix, annot=True ,fmt='g', xticklabels=iris.feature_names, yticklabels=iris.feature_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6xeeRsPzoRe"
      },
      "source": [
        "2c.Calculate the correlation coefficient between sepal length and petal length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CofrYUpBzo_M"
      },
      "outputs": [],
      "source": [
        "sepal_length = X[:, 0]  # Extract sepal length feature\n",
        "petal_length = X[:, 2]  # Extract petal length feature\n",
        "\n",
        "correlation_coefficient = np.corrcoef(sepal_length, petal_length)[0, 1]\n",
        "\n",
        "print(\"Correlation coefficient between sepal length and petal length:\", correlation_coefficient)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oDLJO-74qLQ"
      },
      "source": [
        "\n",
        "The np.corrcoef() function in NumPy is used to calculate the correlation coefficients between multiple variables. It computes the correlation coefficient matrix, which measures the linear relationship between pairs of variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVSt1HN2zr0a"
      },
      "source": [
        "2d.Count the number of occurrences of each unique value in the target variable (species):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MHrZAj0zq0-"
      },
      "outputs": [],
      "source": [
        "unique_species, species_counts = np.unique(y, return_counts=True)\n",
        "\n",
        "print(\"Species counts:\")\n",
        "for species, count in zip(unique_species, species_counts):\n",
        "    print(f\"Species {species}: {count} occurrences\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBV-l1MU4dWF"
      },
      "source": [
        "The np.unique() function in NumPy is used to find the unique elements in an array. It returns a sorted array of unique values without any repetitions. The function takes an array-like object as input and returns a new array containing only the unique elements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGBHprFr7f3F"
      },
      "source": [
        "2e.Normalize feature to have zero mean and unit variance:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For some algorithem, it's essential to normalize the features."
      ],
      "metadata": {
        "id": "WL6Jc3pWbRW3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHEK4UaF7e4N"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.load_dataset(\"iris\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "StandardScaler: Removing the mean and scaling to unit variance.\n",
        "\n",
        "This operation is performed feature-wise in an independent way."
      ],
      "metadata": {
        "id": "B0ADYXSgqr0U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thM1vAMU_Hgn"
      },
      "outputs": [],
      "source": [
        "petal_width = X[:, 3]  # Extract petal width feature ????\n",
        "scale_petal_width = (petal_width - np.mean(petal_width)) / np.std(petal_width)\n",
        "\n",
        "print(\"Scale petal width:\")\n",
        "print(scale_petal_width)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dW9Ei4j74aE"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# StandardScaler\n",
        "scalar = StandardScaler()\n",
        "scalar.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "StandardScaler can be influenced by outliers\n"
      ],
      "metadata": {
        "id": "AbKfgzG5rYWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X[0]"
      ],
      "metadata": {
        "id": "VS49eZy3tRj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zb4izDhZ77_O"
      },
      "outputs": [],
      "source": [
        "# Normalization\n",
        "from sklearn.preprocessing import Normalizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This normalization technique is often used when the direction of the data points is more important than their absolute values. It's common in various machine learning algorithms, especially those that rely on similarity measures or distances between data points, such as cosine similarity.\n",
        "\n",
        "By normalizing samples to unit norm, you ensure that the differences in scale among samples do not dominate the analysis or algorithms, and you focus more on the relationships between directions of vectors rather than their magnitudes.\n",
        "\n",
        " columns that have large values will dominate the distance measure.\n",
        "\n",
        " normalization is crucial for the following reasons: 1. Distance-based Metrics: KNN relies on distance-based metrics"
      ],
      "metadata": {
        "id": "ofWHLRk1s-KE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4URZ7Tg97-X7"
      },
      "outputs": [],
      "source": [
        "# Normalizer\n",
        "norm = Normalizer()\n",
        "X = norm.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "o8qwUqFitDGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do we need to standardize/ Normalize variables before doing correlation analysis?"
      ],
      "metadata": {
        "id": "wBLZNiNKt20C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3oI2vNy8Mx7"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "Image(url='https://media.tenor.com/P4nq8scOcbYAAAAC/kitty-no.gif')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqL9whAXFEHw"
      },
      "source": [
        "The correlation coefficient is independent of change of origin and scale"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Popular algorithms for multi-class classification:\n",
        "\n",
        "k-Nearest Neighbors\n",
        "\n",
        "Decision Trees\n",
        "\n",
        "Naive Bayes\n",
        "\n",
        "Random Forest\n",
        "\n",
        "Gradient Boosting"
      ],
      "metadata": {
        "id": "gAkxdKyWyOO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Nearest Neighbour (KNN)\n",
        "\n",
        "\n",
        "Simple supervised algorithm used for the regression but especially more for the classification.\n",
        "\n",
        "KNN is a lazy learner who does not learn a discriminative function from the training dataset. Instead, it stores the dataset, and at the classification time, it performs actions on the dataset.\n",
        "\n",
        "The core idea behind KNN is to find a similarity between the new data and the available data.\n",
        "\n",
        "A data point is classified based on the plurality vote of its neighbors, namely the most common label among the nearest neighbors.\n",
        "\n",
        "Symbol K in KNN refers to the number of nearest neighbors included in the classification process.\n",
        "\n",
        "A low value causes skewed classification, while a large value raises difficulties in searching the nearest neighbors for samples and resource issues."
      ],
      "metadata": {
        "id": "1TKABURh0NQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Itâ€™s popular in many fields, including:\n",
        "\n",
        "1) Computer Vision\n",
        "\n",
        "2) Content Recommendation"
      ],
      "metadata": {
        "id": "sX92yp6mU0_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "KNN can be used for both classification and regression predictive problems. However, it is more widely used in classification problems in the industry.\n"
      ],
      "metadata": {
        "id": "G-IZjW-GABcU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How Does the KNN Algorithm Work?\n"
      ],
      "metadata": {
        "id": "HyIGAACXVfqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "distances and similarities\n"
      ],
      "metadata": {
        "id": "rGe7jK-A6Y2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image(url=\"https://www.gstatic.com/education/formulas2/553212783/en/euclidean_distance.svg\")"
      ],
      "metadata": {
        "id": "VuFU-yAswiBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def e_dist(point1, point2):\n",
        "  sum_sq = np.sqrt(np.sum(np.square(point1 - point2)))\n",
        "  return sum_sq\n",
        "\n",
        "point1 = np.array((1, 1, 1))\n",
        "point2 = np.array((1, 1, 1))\n",
        "e_dist(point1, point2)"
      ],
      "metadata": {
        "id": "nquegV-yx-iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1 / (1 + e_dist(point1, point2))"
      ],
      "metadata": {
        "id": "ghK6arr1yKxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "euclidean_distances(X)"
      ],
      "metadata": {
        "id": "0CMooKG9uIbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Overfitting: the model gives accurate predictions for training but not for testing.\n",
        "\n",
        "Underfitting:the model has not learned the patterns in the training data well"
      ],
      "metadata": {
        "id": "CWdMGFnB8dHy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5wWgIwphIR3"
      },
      "outputs": [],
      "source": [
        "# Lets train a simple model\n",
        "from IPython.display import Image\n",
        "Image(url='https://media.tenor.com/e-LsbnNHQ5cAAAAd/catjam-cat-dancing.gif')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IDEudzI91PR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlUTCBo5hZKf"
      },
      "outputs": [],
      "source": [
        "# Create train and test dataset\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets make it clear"
      ],
      "metadata": {
        "id": "aTaJCRtv8Lxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image(url=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H2UmG5L1I5bzFCW006N5Ag.png\")"
      ],
      "metadata": {
        "id": "2tzz1hUU8DVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_size = 0.30"
      ],
      "metadata": {
        "id": "a_gUjTdEMKvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEMUiNtwpVEw"
      },
      "outputs": [],
      "source": [
        "# random_state: controls the shuffling proces\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "X, y, test_size=t_size, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFWOrHsZji2D"
      },
      "outputs": [],
      "source": [
        "# check the shapes\n",
        "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWfHMGnUjkM7"
      },
      "outputs": [],
      "source": [
        "# See if datasets have same population\n",
        "np.unique(y_train, return_counts=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DExR8xm5jlDd"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "X, y, test_size=t_size, random_state=0, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3l2u_SAjmFX"
      },
      "outputs": [],
      "source": [
        "np.unique(y_train, return_counts=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6amMlbejnbs"
      },
      "outputs": [],
      "source": [
        "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCAm3v8ajoMK"
      },
      "outputs": [],
      "source": [
        "np.unique(y_test, return_counts=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_classifier.fit(x_train, y_train)\n",
        "knn_classifier.score(x_test, y_test) # to find score\n",
        "pred_test_knn = knn_classifier.predict(x_test)\n",
        "pred_test_knn"
      ],
      "metadata": {
        "id": "GTpA-cs2B_6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test[:,0]"
      ],
      "metadata": {
        "id": "FHdyDIQYNV1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test"
      ],
      "metadata": {
        "id": "LvExbthuNNT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test,pred_test_knn)\n",
        "sns.heatmap(cm,annot=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "8enQdSuzLrhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_test_knn = knn_classifier.predict_proba(x_test)\n",
        "pred_test_knn"
      ],
      "metadata": {
        "id": "JTHHy35VB_xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross validation (CV)"
      ],
      "metadata": {
        "id": "HrpWWDNEYnni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image(url=\"https://www.section.io/engineering-education/how-to-implement-k-fold-cross-validation/5-fold-cv.jpeg\")"
      ],
      "metadata": {
        "id": "bjx8V1zLYJxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main purpose of cross validation is to prevent overfitting"
      ],
      "metadata": {
        "id": "90OYL01aYp6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "def KNN():\n",
        "    knn_grid = {\n",
        "        \"n_neighbors\": list(range(1, 10, 1)),\n",
        "        \"weights\": [\"distance\"],\n",
        "        \"metric\": [\"euclidean\"],\n",
        "    }\n",
        "\n",
        "    knn = KNeighborsClassifier()\n",
        "    # grid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy', return_train_score=False,verbose=1)\n",
        "\n",
        "    return GridSearchCV(\n",
        "        estimator = knn,\n",
        "        param_grid = knn_grid,\n",
        "        cv=10,\n",
        "        n_jobs=1,\n",
        "        scoring=\"accuracy\",\n",
        "    )\n",
        "model = KNN()\n",
        "model.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "pnPjn8sxB_ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.best_estimator_"
      ],
      "metadata": {
        "id": "2A99X72yHwLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.best_score_"
      ],
      "metadata": {
        "id": "v_vDpPb3Hyyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.best_params_"
      ],
      "metadata": {
        "id": "i79q0_3ZH3iC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_test_knn = model.best_estimator_.predict(x_test)\n",
        "pred_test_knn"
      ],
      "metadata": {
        "id": "iFFGNdd3ILja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test,pred_test_knn)\n",
        "sns.heatmap(cm,annot=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "bz1GXoCsJtbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the whole report\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, pred_test_knn))"
      ],
      "metadata": {
        "id": "FUBFp3U9L_M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEhdiNyICXrT"
      },
      "outputs": [],
      "source": [
        "x_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nu_rR8PCaXs"
      },
      "outputs": [],
      "source": [
        "x_test[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITU73yHGCjSK"
      },
      "outputs": [],
      "source": [
        "id = 0\n",
        "model.best_estimator_.predict([x_test[id]]), y_test[id]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advantages\n",
        "\n",
        "1) Easy to implement\n",
        "\n",
        "2) Few hyperparameters\n"
      ],
      "metadata": {
        "id": "oUs01V_061zt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Disadvantages\n",
        "\n",
        "1) Does not scale well: Since KNN is a lazy algorithm\n",
        "\n",
        "2) Prone to overfitting"
      ],
      "metadata": {
        "id": "jhsn7Eq364iN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustering\n",
        "\n",
        "Clustering means grouping"
      ],
      "metadata": {
        "id": "H8de1WfqZFKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KMeans\n"
      ],
      "metadata": {
        "id": "2QUTm1rJ9VDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image(url=\"https://www.saedsayad.com/images/Clustering_kmeans_c.png\")"
      ],
      "metadata": {
        "id": "Bj8GYrmd8DGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algorithm\n",
        "\n",
        "  1) Clusters the data into k groups\n",
        "    \n",
        "  2) Assign objects to their closest cluster center according to the Euclidean distance function.\n",
        "    \n",
        "  4) Calculate the centroid or mean of all objects in each cluster.\n",
        "    \n",
        "  5) Repeat steps 2 and 3 until the same points are assigned to each cluster in consecutive rounds.\n"
      ],
      "metadata": {
        "id": "V_TkYoEt8LKu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/datasets/vetrirah/customer"
      ],
      "metadata": {
        "id": "TPCa9Tde-iqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"Train.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "jVN_0VcO-lpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop([\"Segmentation\", \"ID\"], axis=\"columns\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "SMNfMt6B_Zxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "k1DRBumU_omr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()\n",
        "df"
      ],
      "metadata": {
        "id": "iij512h__2w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Any issue?"
      ],
      "metadata": {
        "id": "dzDR5ZLFA6Gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "b_NlFdK8_51D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "dkmD9B7dAMuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "BLWTwP0lAYox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Any issue?"
      ],
      "metadata": {
        "id": "7qfZEko6A5A5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(\"index\", axis=\"columns\", inplace=True)"
      ],
      "metadata": {
        "id": "1C4HENZFBA8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "PBoTWQ3VBYZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1 = pd.get_dummies(df)\n",
        "df_1.head()"
      ],
      "metadata": {
        "id": "Xms2F8zIBhkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1 = pd.get_dummies(df, drop_first=True)\n",
        "df_1"
      ],
      "metadata": {
        "id": "CtH5Q5eCB9af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "Scaler = StandardScaler()\n",
        "Scaled_df_kmeans = Scaler.fit_transform(df_1)\n",
        "Scaled_df_kmeans"
      ],
      "metadata": {
        "id": "_e7WlsXDCsq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizer\n",
        "norm = Normalizer()\n",
        "X = norm.fit_transform(df_1)\n",
        "X"
      ],
      "metadata": {
        "id": "rc_fyDHvCdzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "Kmeans_model = KMeans(n_clusters=3)\n",
        "Clusters = Kmeans_model.fit_predict(Scaled_df_kmeans)\n",
        "df_1[\"Cluster_1\"] = Clusters\n",
        "df_1.head(3)"
      ],
      "metadata": {
        "id": "Rbje_LmhDbDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1.columns"
      ],
      "metadata": {
        "id": "Hn2OA0s1FQ8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1.insert(df_1.columns.get_loc(\"Age\"), \"Cluster_Sca\", Clusters)\n",
        "df_1"
      ],
      "metadata": {
        "id": "Ei03kdxbFF55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "Kmeans_model = KMeans(n_clusters=3)\n",
        "Clusters = Kmeans_model.fit_predict(X)\n",
        "df_1.insert(df_1.columns.get_loc(\"Age\"), \"Cluster_Nor\", Clusters)\n",
        "df_1.head()"
      ],
      "metadata": {
        "id": "a6hct8NCDlmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv(\"Train.csv\")"
      ],
      "metadata": {
        "id": "ijG_iVvHIhuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to specify the number of clusters, in advance"
      ],
      "metadata": {
        "id": "PlgQWxaQ8S9E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_U4gOEbkx_B"
      },
      "outputs": [],
      "source": [
        "# Practice\n",
        "# 1) Select breast_cancer datasets in sklearn\n",
        "# 2) create features and labels\n",
        "# 3) divide the dataset in this way: 75% for training and 25% for testing. make sure the number of labels are identical\n",
        "# 4) train simple SVM\n",
        "# 5) print test accuracy\n",
        "# 6) plot confusion matrix\n",
        "# 7) print classification report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUg8PmNZn3a8"
      },
      "outputs": [],
      "source": [
        "# 1\n",
        "from sklearn.datasets import ????\n",
        "lin = ????"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2s6bn-Szl2Ix"
      },
      "outputs": [],
      "source": [
        "# 2\n",
        "X = lin.??\n",
        "y = lin.??"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XogaQHcmS4m"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 3\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "??, ??, test_size=??, random_state=0, ??)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0ecqARimVEi"
      },
      "outputs": [],
      "source": [
        "# 4\n",
        "from sklearn.svm import SVC\n",
        "model = ??()\n",
        "model.fit(??, ??)\n",
        "model.predict(??)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XU89c0VPmYm9"
      },
      "outputs": [],
      "source": [
        "# 5\n",
        "model.score(??, ??)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = ?(?,?)\n",
        "sns.?(cm,annot=?)\n",
        "\n"
      ],
      "metadata": {
        "id": "yXMTBzCu5co6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ?\n",
        "print(?(?, ?))"
      ],
      "metadata": {
        "id": "G1Os3Fit5dCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please explain in detail your understanding of the entire activity in **atleast 200 words.**"
      ],
      "metadata": {
        "id": "5HKUNHJqAvgi"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}